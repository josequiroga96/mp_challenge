#!/usr/bin/env bash
# Enhanced AB suite with unique names and comprehensive load testing:
# - Generates unique sequential names to avoid business rule conflicts
# - Multiple test phases: warmup, read, write, mixed, concurrency scaling, spikes, stress
# - No early stop on 4xx/5xx (suite continues always)
# - Records metrics per scenario in CSV (summary.csv)
# - Optional percentiles export (--percentiles) in TSV/CSV (-g/-e)
# - Shows Non-2xx and Failed requests per scenario
# - Optional 2xx/3xx/4xx/5xx breakdown via "code sample" with -v 3 (CODESAMPLE=1)
#
# Requirements: awk, sed, grep, jq (for JSON manipulation)
#
# Environment variables:
#   BASE_URL, OUT, PERCENTILES=1, CODESAMPLE=1, CODE_N=500, TEST_PHASES
#
# Usage:
#   chmod +x run-suite-enhanced.sh
#   ./run-suite-enhanced.sh
#
# Test phases (comma-separated):
#   warmup,read,write,mixed,scaling,spikes,stress
#   Default: warmup,read,write,mixed,scaling
#
# After completion:
#   - $OUT_DIR/summary.csv              (summary per scenario)
#   - $OUT_DIR/raw/<scenario>.txt       (raw ab output)
#   - $OUT_DIR/codes/<scenario>.codes   (only if CODESAMPLE=1)
#   - $OUT_DIR/percentiles/*            (if PERCENTILES=1)
#   - $OUT_DIR/payloads/*               (generated unique payloads)
#
set -u  # no -e: don't abort on 4xx/5xx or parse failures
BASE_URL=${BASE_URL:-"http://localhost:8080"}
OUT=${OUT:-"load-test-results"}
PERCENTILES=${PERCENTILES:-0}
CODESAMPLE=${CODESAMPLE:-0}
CODE_N=${CODE_N:-500}
TEST_PHASES=${TEST_PHASES:-"warmup,read,write,mixed,scaling"}

# Counter for unique names
UNIQUE_COUNTER=1

# Create organized directory structure
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
TEST_TYPE=${TEST_TYPE:-"unknown"}
OUT_DIR="$OUT/$TEST_TYPE-$TIMESTAMP"

# Ensure base directory exists
mkdir -p "$OUT"

# Create test-specific directory
mkdir -p "$OUT_DIR/raw"
mkdir -p "$OUT_DIR/payloads"
[ "$PERCENTILES" = "1" ] && mkdir -p "$OUT_DIR/percentiles"
[ "$CODESAMPLE" = "1" ] && mkdir -p "$OUT_DIR/codes"

SUMMARY="$OUT_DIR/summary.csv"
echo "scenario,concurrency,requests,failed_requests,non2xx,requests_per_sec,time_per_request_ms,p50_ms,p90_ms,p99_ms,transfer_kBps" > "$SUMMARY"

# Generate unique payloads
generate_unique_payloads() {
    echo "Generating unique payloads..."
    
    local timestamp=$(date +%s)
    local current_time=$(date)
    local price_base=$((1000 + UNIQUE_COUNTER * 100))
    local price_update=$((1500 + UNIQUE_COUNTER * 50))
    local price_patch=$((2000 + UNIQUE_COUNTER * 25))
    local rating_base=$((5 + UNIQUE_COUNTER % 5))
    local rating_update=$((7 + UNIQUE_COUNTER % 3))
    
    # Create payload with unique name
    cat > "$OUT_DIR_DIR/payloads/create.json" << EOF
{
  "name": "LoadTest-Item-${timestamp}-${UNIQUE_COUNTER}",
  "description": "Load test item generated at ${current_time}",
  "imageUrl": "https://example.com/loadtest-${UNIQUE_COUNTER}.jpg",
  "price": ${price_base}.99,
  "rating": 4.${rating_base},
  "specifications": ["LoadTest", "AutoGenerated", "Test${UNIQUE_COUNTER}"]
}
EOF

    # Update payload with unique name
    cat > "$OUT_DIR_DIR/payloads/update.json" << EOF
{
  "id": "PLACEHOLDER_ID",
  "name": "Updated-LoadTest-Item-${timestamp}-${UNIQUE_COUNTER}",
  "description": "Updated load test item generated at ${current_time}",
  "imageUrl": "https://example.com/updated-loadtest-${UNIQUE_COUNTER}.jpg",
  "price": ${price_update}.99,
  "rating": 4.${rating_update},
  "specifications": ["Updated", "LoadTest", "AutoGenerated", "Test${UNIQUE_COUNTER}"]
}
EOF

    # Patch payload
    cat > "$OUT_DIR/payloads/patch.json" << EOF
{
  "name": "Patched-LoadTest-Item-${timestamp}-${UNIQUE_COUNTER}",
  "price": ${price_patch}.99
}
EOF

    # Add specs payload
    cat > "$OUT_DIR/payloads/add-specs.json" << EOF
{
  "specifications": ["LoadTestSpec${UNIQUE_COUNTER}", "AutoGenerated", "PerformanceTest", "Concurrency${UNIQUE_COUNTER}"]
}
EOF

    # Batch payload
    cat > "$OUT_DIR/payloads/batch.json" << EOF
{
  "ids": ["PLACEHOLDER_ID_1", "PLACEHOLDER_ID_2", "PLACEHOLDER_ID_3"]
}
EOF

    echo "Unique payloads generated in $OUT_DIR/payloads/"
}

# Get a random existing item ID for update/patch operations
get_random_item_id() {
    curl -s "$BASE_URL/api/items" | jq -r '.[0].id' 2>/dev/null || echo "fallback-id"
}

# Update payloads with actual IDs
update_payloads_with_ids() {
    local item_id=$(get_random_item_id)
    if [ "$item_id" != "fallback-id" ]; then
        # Update the update.json with actual ID
        sed -i.bak "s/PLACEHOLDER_ID/$item_id/g" "$OUT_DIR/payloads/update.json"
        
        # Update batch.json with actual IDs (get 3 random IDs)
        local ids=$(curl -s "$BASE_URL/api/items" | jq -r '.[0:3] | .[].id' | tr '\n' ',' | sed 's/,$//' | sed 's/,/","/g')
        if [ -n "$ids" ]; then
            sed -i.bak "s/PLACEHOLDER_ID_1\",\"PLACEHOLDER_ID_2\",\"PLACEHOLDER_ID_3/\"$ids/g" "$OUT_DIR/payloads/batch.json"
        fi
    fi
}

parse_and_append() {
  # $1 = scenario name, $2 = path to ab output file
  local name="$1"
  local file="$2"
  # Safe default values
  local conc=""
  local requests=""
  local failed="0"
  local non2xx="0"
  local rps=""
  local tpr=""
  local p50=""
  local p90=""
  local p99=""
  local kbps=""

  # Extract metrics
  conc=$(awk -F: '/Concurrency Level/{gsub(/ /,"",$2); print $2}' "$file" 2>/dev/null || echo "")
  requests=$(awk -F: '/Complete requests/{gsub(/ /,"",$2); print $2}' "$file" 2>/dev/null || echo "")
  failed=$(awk -F: '/Failed requests/{gsub(/ /,"",$2); print $2+0}' "$file" 2>/dev/null || echo "0")
  non2xx=$(awk -F: '/Non-2xx responses/{gsub(/ /,"",$2); print $2+0}' "$file" 2>/dev/null || echo "0")
  rps=$(awk -F: '/Requests per second/{match($0,/([0-9]+\.[0-9]+)/,m); if(m[1]!=""){print m[1]}}' "$file" 2>/dev/null || echo "")
  tpr=$(awk -F: '/Time per request:/{match($0,/([0-9]+\.[0-9]+)\s*\[ms\]\s*\(mean\)/,m); if(m[1]!=""){print m[1]} }' "$file" 2>/dev/null | head -n1)
  kbps=$(awk -F: '/Transfer rate/{match($0,/([0-9]+\.[0-9]+)\s*\[Kbytes\/sec\]/,m); if(m[1]!=""){print m[1]}}' "$file" 2>/dev/null || echo "")

  # Percentiles
  p50=$(awk '/ 50%/{print $2}' "$file" 2>/dev/null || echo "")
  p90=$(awk '/ 90%/{print $2}' "$file" 2>/dev/null || echo "")
  p99=$(awk '/ 99%/{print $2}' "$file" 2>/dev/null || echo "")

  echo "$name,$conc,$requests,$failed,$non2xx,$rps,$tpr,$p50,$p90,$p99,$kbps" >> "$SUMMARY"
}

code_sample_breakdown() {
  # $1 = scenario name, $2 = ab URL/args (string), $3 = base options, $4 = payload file (optional), $5 = content-type (optional)
  local name="$1"; shift
  local endpoint="$1"; shift
  local base_opts="$1"; shift
  local payload="${1:-}"; shift || true
  local ctype="${1:-}" || true

  # Build command
  local codes_file="$OUT_DIR/codes/${name}.codes"
  local args=()
  # reduce requests to CODE_N, keep same concurrency from base_opts if present (-c N)
  local conc=$(echo "$base_opts" | awk '{for(i=1;i<=NF;i++){if($i=="-c"){print $(i+1); exit}}}')
  if [ -z "$conc" ]; then conc=10; fi

  args+=(-n "$CODE_N" -c "$conc" -v 3)
  # if keep-alive present in base_opts
  if echo "$base_opts" | grep -q -- " -k "; then args+=(-k); fi
  # payload
  if [ -n "$payload" ] && [ -f "$payload" ]; then
    args+=(-p "$payload")
    if [ -n "$ctype" ]; then
      args+=(-T "$ctype")
    fi
  fi

  # Run code-sample and capture verbose output (headers + status lines)
  ab "${args[@]}" "$endpoint" > "$codes_file" 2>&1 || true

  # Count status classes
  local c2xx=$(grep -Eo 'HTTP/[0-9.]+\s+2[0-9]{2}' "$codes_file" | wc -l | tr -d ' ')
  local c3xx=$(grep -Eo 'HTTP/[0-9.]+\s+3[0-9]{2}' "$codes_file" | wc -l | tr -d ' ')
  local c4xx=$(grep -Eo 'HTTP/[0-9.]+\s+4[0-9]{2}' "$codes_file" | wc -l | tr -d ' ')
  local c5xx=$(grep -Eo 'HTTP/[0-9.]+\s+5[0-9]{2}' "$codes_file" | wc -l | tr -d ' ')

  echo "  â†³ CodeSample ($CODE_N reqs) => 2xx=$c2xx, 3xx=$c3xx, 4xx=$c4xx, 5xx=$c5xx"
}

run() {
  # name + full ab args including URL
  local name="$1"; shift
  local out="$OUT_DIR/raw/${name}.txt"
  echo "==> $name"
  # Run AB (no exit on errors), capture to file
  ab "$@" > "$out" 2>&1 || true

  # Parse and append to summary
  parse_and_append "$name" "$out"

  # Optional: export percentiles data
  if [ "$PERCENTILES" = "1" ]; then
    # (Optional) User can re-run endpoints with -g/-e for graphs.
    :
  fi
}

# ------------------ Test Phases ------------------

warmup_tests() {
    echo "=== WARMUP PHASE ==="
    run "00_warmup_health"  -k -n 500  -c 20  "$BASE_URL/public/health"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "00_warmup_health" "$BASE_URL/public/health" " -k -c 20 "
    
    run "01_warmup_list"    -k -n 1000 -c 50  "$BASE_URL/api/items"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "01_warmup_list" "$BASE_URL/api/items" " -k -c 50 "
}

read_tests() {
    echo "=== READ PHASE ==="
    run "10_read_items"     -k -n 10000 -c 100 "$BASE_URL/api/items"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "10_read_items" "$BASE_URL/api/items" " -k -c 100 "
    
    run "11_read_count"     -k -n 8000  -c 100 "$BASE_URL/api/items/count"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "11_read_count" "$BASE_URL/api/items/count" " -k -c 100 "
    
    run "12_read_search_nm" -k -n 8000  -c 100 "$BASE_URL/api/items/search?name=laptop"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "12_read_search_nm" "$BASE_URL/api/items/search?name=laptop" " -k -c 100 "
    
    run "13_read_search_rt" -k -n 8000  -c 100 "$BASE_URL/api/items/search/rating?minRating=4"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "13_read_search_rt" "$BASE_URL/api/items/search/rating?minRating=4" " -k -c 100 "
    
    run "14_read_search_pr" -k -n 8000  -c 100 "$BASE_URL/api/items/search/price?minPrice=500&maxPrice=2000"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "14_read_search_pr" "$BASE_URL/api/items/search/price?minPrice=500&maxPrice=2000" " -k -c 100 "
}

write_tests() {
    echo "=== WRITE PHASE ==="
    # Generate unique payloads for this phase
    generate_unique_payloads
    update_payloads_with_ids
    
    run "20_write_create"   -k -n 3000 -c 50  -p "$OUT_DIR/payloads/create.json" -T "application/json" "$BASE_URL/api/items"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "20_write_create" "$BASE_URL/api/items" " -k -c 50 " "$OUT_DIR/payloads/create.json" "application/json"
    
    run "21_write_update"   -k -n 2000 -c 50  -p "$OUT_DIR/payloads/update.json" -T "application/json" "$BASE_URL/api/items/$(get_random_item_id)"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "21_write_update" "$BASE_URL/api/items/$(get_random_item_id)" " -k -c 50 " "$OUT_DIR/payloads/update.json" "application/json"
    
    run "22_write_patch"    -k -n 2000 -c 50  -p "$OUT_DIR/payloads/patch.json"  -T "application/json" "$BASE_URL/api/items/$(get_random_item_id)"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "22_write_patch" "$BASE_URL/api/items/$(get_random_item_id)" " -k -c 50 " "$OUT_DIR/payloads/patch.json" "application/json"
    
    run "23_write_addspecs" -k -n 2000 -c 50  -p "$OUT_DIR/payloads/add-specs.json" -T "application/json" "$BASE_URL/api/items/$(get_random_item_id)/specifications"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "23_write_addspecs" "$BASE_URL/api/items/$(get_random_item_id)/specifications" " -k -c 50 " "$OUT_DIR/payloads/add-specs.json" "application/json"
    
    run "24_read_batch"     -k -n 6000 -c 80   -p "$OUT_DIR/payloads/batch.json" -T "application/json" "$BASE_URL/api/items/batch"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "24_read_batch" "$BASE_URL/api/items/batch" " -k -c 80 " "$OUT_DIR/payloads/batch.json" "application/json"
    
    run "25_write_delete"   -k -n 1000 -c 25   "$BASE_URL/api/items/$(get_random_item_id)"
    [ "$CODESAMPLE" = "1" ] && code_sample_breakdown "25_write_delete" "$BASE_URL/api/items/$(get_random_item_id)" " -k -c 25 "
}

mixed_tests() {
    echo "=== MIXED PHASE (Read/Write combination) ==="
    # Mixed workload: 70% reads, 30% writes
    run "30_mixed_reads"    -k -n 7000 -c 100 "$BASE_URL/api/items"
    run "31_mixed_writes"   -k -n 3000 -c 50  -p "$OUT_DIR/payloads/create.json" -T "application/json" "$BASE_URL/api/items"
    
    # Mixed search operations
    run "32_mixed_search"   -k -n 5000 -c 80  "$BASE_URL/api/items/search?name=LoadTest"
    run "33_mixed_updates"  -k -n 2000 -c 40  -p "$OUT_DIR/payloads/patch.json" -T "application/json" "$BASE_URL/api/items/$(get_random_item_id)"
}

scaling_tests() {
    echo "=== CONCURRENCY SCALING PHASE ==="
    # Test different concurrency levels
    run "40_scale_c10"      -k -n 2000 -c 10  "$BASE_URL/api/items"
    run "41_scale_c25"      -k -n 2000 -c 25  "$BASE_URL/api/items"
    run "42_scale_c50"      -k -n 2000 -c 50  "$BASE_URL/api/items"
    run "43_scale_c100"     -k -n 2000 -c 100 "$BASE_URL/api/items"
    run "44_scale_c200"     -k -n 2000 -c 200 "$BASE_URL/api/items"
    run "45_scale_c500"     -k -n 2000 -c 500 "$BASE_URL/api/items"
    
    # Write scaling
    run "46_scale_write_c10"  -k -n 1000 -c 10  -p "$OUT_DIR/payloads/create.json" -T "application/json" "$BASE_URL/api/items"
    run "47_scale_write_c50"  -k -n 1000 -c 50  -p "$OUT_DIR/payloads/create.json" -T "application/json" "$BASE_URL/api/items"
    run "48_scale_write_c100" -k -n 1000 -c 100 -p "$OUT_DIR/payloads/create.json" -T "application/json" "$BASE_URL/api/items"
}

spike_tests() {
    echo "=== SPIKE TESTING PHASE ==="
    # Sudden spikes in traffic
    run "50_spike_normal"   -k -n 1000 -c 50  "$BASE_URL/api/items"
    run "51_spike_high"     -k -n 5000 -c 500 "$BASE_URL/api/items"
    run "52_spike_normal"   -k -n 1000 -c 50  "$BASE_URL/api/items"
    run "53_spike_extreme"  -k -n 10000 -c 1000 "$BASE_URL/api/items"
    run "54_spike_normal"   -k -n 1000 -c 50  "$BASE_URL/api/items"
}

stress_tests() {
    echo "=== STRESS TESTING PHASE ==="
    # Push the system to its limits
    run "60_stress_reads"   -k -n 20000 -c 1000 "$BASE_URL/api/items"
    run "61_stress_writes"  -k -n 10000 -c 500  -p "$OUT_DIR/payloads/create.json" -T "application/json" "$BASE_URL/api/items"
    run "62_stress_mixed"   -k -n 30000 -c 2000 "$BASE_URL/api/items"
    run "63_stress_search"  -k -n 15000 -c 1000 "$BASE_URL/api/items/search?name=LoadTest"
}

# ------------------ Main Execution ------------------

echo "Starting Enhanced Load Test Suite"
echo "Test phases: $TEST_PHASES"
echo "Output directory: $OUT_DIR"
echo "Base URL: $BASE_URL"
echo ""

# Generate initial unique payloads
generate_unique_payloads
update_payloads_with_ids

# Run selected test phases
IFS=',' read -ra PHASES <<< "$TEST_PHASES"
for phase in "${PHASES[@]}"; do
    case "$phase" in
        "warmup")
            warmup_tests
            ;;
        "read")
            read_tests
            ;;
        "write")
            write_tests
            ;;
        "mixed")
            mixed_tests
            ;;
        "scaling")
            scaling_tests
            ;;
        "spikes")
            spike_tests
            ;;
        "stress")
            stress_tests
            ;;
        *)
            echo "Unknown test phase: $phase"
            ;;
    esac
    echo ""
done

echo "------------------------------------------------------------"
echo "DONE -> $OUT_DIR"
echo "Summary: $SUMMARY"
[ "$CODESAMPLE" = "1" ] && echo "Status code breakdown in: $OUT_DIR/codes/*.codes"
echo "Generated payloads in: $OUT_DIR/payloads/"
echo ""
echo "All results organized in: $OUT/"
